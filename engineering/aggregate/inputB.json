{
    "_id": 2,
    "title":"Asynchronous Web Scraping With Python AIOHTTP",
    "author":"Ronnie Atuhaire",
    "date_published":"2019-05-21T16:21:43.081Z",
    "lead_image_url":"https://hashnode.com/utility/r?url=https%3A%2F%2Fcdn.hashnode.com%2Fres%2Fhashnode%2Fimage%2Fupload%2Fv1648740784197%2FacHwqvSOX.png%3Fw%3D1200%26h%3D630%26fit%3Dcrop%26crop%3Dentropy%26auto%3Dcompress%2Cformat%26format%3Dwebp%26fm%3Dpng",
    "content":"Hey there ðŸ‘‹, welcome here! Having looked at Asynchronous Web Scraping With Python GRequests, today we are using a different approach as I promised; We are using aiohttp\nOpen that article in a new tab because I will be referencing to it.\nSo we shall be using two major modules and one comes with the standard python library. It is a library for building web-client and web-server using Python and asyncio. It is a Python 3â€™s built-in library. This means itâ€™s already installed if you have Python 3. Since Python 3.5, it is convenient to work with asynchronous code.\nasyncio stands for Asynchronous Input-Output. This is a very powerful concept to use whenever you work IO. Interacting with the web or external APIs such as Telegram makes a lot of sense this way.\nLet's not waste time and import the necessary modules\nimport aiohttp\nimport asyncio\nfrom timeit import default_timer\nYou may want to pip install aiohttp before you continue if you don't have it.\nWe define the same and exact URLs we used in our synchronous file in this article;\nurls = ['https: 'https: 'https: 'https: 'https: 'https:\nNow create a normal function that has async pre-appended to it lie this; async def main(): and add the following code underneath which I will explain later on;\nThis function will help us fetch the HTTP Status responses and we shall later define a main() one that will await the results from this function and aid us in creating and running the event loop.\nasync def fetch_status(): start_time = default_timer() async with ClientSession() as session: for url in urls: async with session.get(url) as response: print(f\"[+] Getting Link [+] {url} === {response.status} \") time_elapsed = default_timer() - start_time print(\"It took --- {} seconds --- for all the links\"\n      .format(time_elapsed))\nIn order to have an asynchronous function, we use the async keyword. \ndefault_timer() : This will return the default time when executed.\nWe open a client session with a with keyword that automatically handles the opening & closure of the session for us and then we loop through the URLs to get the response status.\nWe later calculate the time that has elapsed in doing so.\nNow let's create the main() function since it is required and very essential and it will basically call fetch_status().\nasync def main(): await fetch_status()\nNow if everything is good, we create the event loop and run our file;\nloop = asyncio.get_event_loop()\nloop.run_until_complete(main())\nTo run an async function (coroutine) you have to call it using an Event Loop. Event Loops: You can think of Event Loops as functions to run asynchronous tasks and callbacks, perform network IO operations, and run subprocesses.\nRunning the above script produces;\n\nAt the time of writing, I am using Python 3.10 and yours could be perfect (between 3.5-3-8) and you may not see those depreciation warnings above.\nBut if you don't want to see them when you run your file, you can add this on top;\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nSo our entire file is;\nimport aiohttp\nimport asyncio\nfrom timeit import default_timer\nfrom aiohttp import ClientSession\nimport warnings\nwarnings.filterwarnings(\"ignore\") urls = ['https://nytimes.com', 'https://github.com', 'https://google.com', 'https://reddit.com', 'https://hashnode.com', 'https://producthunt.com'] async def fetch_status(): start_time = default_timer() async with ClientSession() as session: for url in urls: async with session.get(url) as response: print(f\"[+] Getting Link [+] {url} === {response.status} \") time_elapsed = default_timer() - start_time print(\"It took --- {} seconds --- for all the links\" .format(time_elapsed)) async def main(): await fetch_status()\n\nloop = asyncio.get_event_loop()\nloop.run_until_complete(main())\n\nThat's it! Find the GitHub repo here.\nðŸ“Œ Read my first article here.\nðŸ“Œ asyncio official docs\nðŸ“Œ aiohttp official docs Once again, hope you learned something today from my little closet.\nPlease consider subscribing or following me for related content, especially about Tech, Python & General Programming.\nYou can show extra love by buying me a coffee to support this free content and I am also open to partnerships, technical writing roles, collaborations and Python-related training or roles.\n  ðŸ“¢ You can also follow me on Twitter : â™¥ â™¥ Waiting for you! ðŸ™‚\n ",
    "next_page_url":null,
    "url":"https://blog.octachart.com/asynchronous-web-scraping-with-python-aiohttp",
    "domain":"blog.octachart.com",
    "excerpt":"Hey there ðŸ‘‹, welcome here! Having looked at Asynchronous Web Scraping With Python GRequests, today we are using a different approach as I promised; We are using aiohttp Open that article in a new tab&hellip;",
    "word_count":671,
    "direction":"ltr",
    "total_pages":1,
    "rendered_pages":1,
    "keywords": null
 }