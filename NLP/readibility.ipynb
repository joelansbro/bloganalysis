{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readability\n",
    "\n",
    "This one takes in readability into account, finds the Flesch reading ease and gunning fog reading and saves it to a column.\n",
    "\n",
    "This blog also finds grammar mistakes via the Language Tool library and saves them to a column also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType, IntegerType\n",
    "import language_tool_python as ltp\n",
    "import textstat\n",
    "\n",
    "# Set up Spark\n",
    "conf = SparkConf().setAppName(\"Article Clarity Metrics\")\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "# Read in Parquet file containing article text\n",
    "df = spark.read.parquet(\"path/to/articles.parquet\")\n",
    "\n",
    "# Define functions to calculate readability scores and grammar mistakes\n",
    "flesch_reading_ease = udf(lambda text: textstat.flesch_reading_ease(text), FloatType())\n",
    "gunning_fog = udf(lambda text: textstat.gunning_fog(text), FloatType())\n",
    "tool = ltp.LanguageTool('en-US')\n",
    "grammar_mistakes = udf(lambda text: len(tool.check(text)), IntegerType())\n",
    "\n",
    "# Add columns for readability scores and grammar mistakes\n",
    "df = df.withColumn(\"flesch_reading_ease\", flesch_reading_ease(\"text\"))\n",
    "df = df.withColumn(\"gunning_fog_index\", gunning_fog(\"text\"))\n",
    "df = df.withColumn(\"grammar_mistakes\", grammar_mistakes(\"text\"))\n",
    "\n",
    "# Save results as Parquet file\n",
    "df.write.parquet(\"path/to/results.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one below is from Bard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "def readability(df):\n",
    "  # Compute the Flesch reading ease score for each article.\n",
    "  df[\"flesch_reading_ease\"] = df[\"words\"].toDouble() / df[\"sentences\"].toDouble()\n",
    "\n",
    "  # Compute the Gunning fog index for each article.\n",
    "  df[\"gunning_fog_index\"] = 2 * df[\"words\"].toDouble() / df[\"sentences\"].toDouble() + 12\n",
    "\n",
    "  # Rank the articles by their readability score.\n",
    "  df.sortBy(df[\"flesch_reading_ease\"], ascending=False).show()\n",
    "\n",
    "def main(args):\n",
    "  # Create a SparkSession.\n",
    "  spark = SparkSession.builder().master(\"local\").appName(\"readability\").build()\n",
    "\n",
    "  # Create a DataFrame of words.\n",
    "  words = spark.createDataFrame(\n",
    "    [\n",
    "      \"The\", \"quick\", \"brown\", \"fox\", \"jumped\", \"over\", \"the\", \"lazy\", \"dog\"\n",
    "    ]\n",
    "  )\n",
    "\n",
    "  # Compute the Flesch reading ease score for each article.\n",
    "  words.withColumn(\"sentences\", words.words.split(\" \").count()).withColumn(\"flesch_reading_ease\", words.words.toDouble() / words.sentences.toDouble())\n",
    "\n",
    "  # Compute the Gunning fog index for each article.\n",
    "  words.withColumn(\"gunning_fog_index\", 2 * words.words.toDouble() / words.sentences.toDouble() + 12)\n",
    "\n",
    "  # Rank the articles by their readability score.\n",
    "  words.sortBy(words[\"flesch_reading_ease\"], ascending=False).show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further ones to check out that may be useful:\n",
    "\n",
    " - DependencyParserModel - analyzes grammatical structure\n",
    " - ChunkerModel - groups related words in a sentence\n",
    " - SentenceDetectorDLModel - sentence boundary detection to split text into individual sentences, to prepare data for named entity recognition and sentiment analysis\n",
    " - NERDLModel - identify named entities\n",
    " - SentimentDLModel - sentiment analysis for sentences positive, negative or neutral\n",
    " - MultiClassifierDLModel - multi-class specification to classify text data into multiple categories, classifies text data into topics, genres, or other sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
