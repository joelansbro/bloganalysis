{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof Of Concept to obtain keywords from different texts\n",
    "\n",
    "We can take advantage of SpaCy's NLP pipeline modules to preselect NLP models for text extraction\n",
    "\n",
    "[Find Modules Here](https://spacy.io/usage/models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/single.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data['content']\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Amazon Web Services, Lambda, 3, One, Lambda Functions, APM, AWS Lambda, AWS, first, 1 million, Lambda, Lambda, Lambda, Create, Lambda, RequestId, 62fa2f25-669c-47b7, RequestId, 62fa2f25-669c-47b7, RequestId, 62fa2f25-669c-47b7, 0.30, 1, 128, 43, 1.34, 62fa2f25-669c-47b7, Sentry, Start Setup, Lambda, Lambda, Lambda Layer, Lambda Layers, Layers, ARN, ARN, US, one, Lambda Layer, Sentry, Lambda, Lambda, APM, Lambda, AWS Lambda, Sentry, DSN, Lambda, Edit, DSN, Sentry, Lambda, sentry_sdk.integrations.aws_lambda, sentry_sdk.init, traces_sample_rate=1.0, 1000, start_transaction(op=\"task, 1000, first, Sentry, Sentry, Next, 3, @mattmakai, GitHub, GitHub)\n"
     ]
    }
   ],
   "source": [
    "# returns a tuple\n",
    "print(doc.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the entities themselves contain multiple keyword duplicates, so below I convert the entities from Spacy into strings and then operate on them with normal python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputList = []\n",
    "for ent in doc.ents:\n",
    "    outputList.append(ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for element in outputList:\n",
    "    if element not in result:\n",
    "        result.append(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Amazon Web Services', 'Lambda', '3', 'One', 'Lambda Functions', 'APM', 'AWS Lambda', 'AWS', 'first', '1 million', 'Create', 'RequestId', '62fa2f25-669c-47b7', '0.30', '1', '128', '43', '1.34', 'Sentry', 'Start Setup', 'Lambda Layer', 'Lambda Layers', 'Layers', 'ARN', 'US', 'one', 'DSN', 'Edit', 'sentry_sdk.integrations.aws_lambda', 'sentry_sdk.init', 'traces_sample_rate=1.0', '1000', 'start_transaction(op=\"task', 'Next', '@mattmakai', 'GitHub']\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "842fbf2151771c5fab737f7e91cae4a6b3fd587e0ef9c8eecff0f9eb53e93c36"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
