{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Markers\n",
    "\n",
    "From Williams and Buchan\n",
    "\n",
    "This script takes in the following metrics:\n",
    "\n",
    "1) frequency of experience markers (I me, we, my, experience)\n",
    "2) temporal events\n",
    "3) presence of verbs in base form\n",
    "4) past tense verbs and gerund verbs\n",
    "5) bigrams where first word is I\n",
    "6) presence of pronouns\n",
    "\n",
    "And rewards points to an article based upon the presence of the markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import col, size, regexp_extract, sum, when, lag, concat_ws\n",
    "import nltk\n",
    "\n",
    "# Set up Spark\n",
    "conf = SparkConf().setAppName(\"Blog Metrics\")\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "# Read in the blog article and split into words\n",
    "text_file = spark.read.text(\"path/to/blog_article.txt\")\n",
    "words = text_file.select(regexp_extract(col(\"value\"), \"\\\\w+\", 0).alias(\"word\"))\n",
    "\n",
    "# Calculate frequency of experience markers (I, me, we, my, experience)\n",
    "experience_markers = [\"i\", \"me\", \"we\", \"my\", \"experience\"]\n",
    "experience_count = words.filter(col(\"word\").isin(experience_markers)).count()\n",
    "\n",
    "# Identify temporal events\n",
    "temporal_markers = [\"yesterday\", \"today\", \"tomorrow\", \"last\", \"this\", \"next\"]\n",
    "temporal_count = words.filter(col(\"word\").isin(temporal_markers)).count()\n",
    "\n",
    "# Find presence of verbs in base form\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "pos = nltk.pos_tag(words.collect())\n",
    "base_verb_count = len([w for w, t in pos if t.startswith(\"VB\")])\n",
    "\n",
    "# Count past tense and gerund verbs\n",
    "past_tense_count = words.filter(col(\"word\").like(\"%ed\")).count()\n",
    "gerund_count = words.filter(col(\"word\").like(\"%ing\")).count()\n",
    "\n",
    "# Identify bigrams where the first word is \"I\"\n",
    "bigrams = words.select(lag(\"word\", 1).over(Window.orderBy(\"word\")).alias(\"prev\"), \"word\")\n",
    "i_bigram_count = bigrams.filter(col(\"prev\") == \"i\").count()\n",
    "\n",
    "# Count presence of pronouns\n",
    "pronoun_count = len([w for w, t in pos if t == \"PRP\"])\n",
    "\n",
    "# Calculate total score\n",
    "score = (experience_count * 2 + temporal_count + base_verb_count +\n",
    "         past_tense_count + gerund_count * 1.5 + i_bigram_count * 2 + pronoun_count * 0.5)\n",
    "\n",
    "# Print score\n",
    "print(f\"Blog score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can calculate the frequency of the experience markers (I, me, we, my, experience) by filtering the words for those specific markers and counting their occurrences\n",
    "\n",
    "To identify temporal events, we will look for words that indicate a specific time period (e.g., \"yesterday\", \"last year\") and count their occurrences:\n",
    "\n",
    "To find the presence of verbs in base form, we will use part-of-speech tagging. We will first need to download the Natural Language Toolkit (NLTK) library, which provides a variety of natural language processing tools, including a part-of-speech tagger.\n",
    "\n",
    "For past tense and gerund verbs, we will count the occurrences of the past tense and gerund forms of verbs:\n",
    "\n",
    "\n",
    "To identify bigrams where the first word is \"I\", we will use a sliding window approach to create pairs of adjacent words and count the occurrences of bigrams where the first word is \"I\":\n",
    "\n",
    "Finally, to count the presence of pronouns, we will count the occurrences of words that are tagged as pronouns using the NLTK part-of-speech tagger:\n",
    "\n",
    "Now that we have all the necessary counts, we can calculate the total score for the article:\n",
    "\n",
    "The weights used in the score calculation are arbitrary and can be adjusted based on the desired importance of each metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
